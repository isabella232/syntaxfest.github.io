<!DOCTYPE html>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Syntax Fest 2019</title>

<link type="text/css" rel="stylesheet" href="syntaxfest19_files/style.css" media="screen">
<link type="text/css" rel="stylesheet" href="syntaxfest19_files/stilesvg.css" media="screen">
<script type="text/javascript" src="syntaxfest19_files/jquery-3.js"></script>
<script type="text/javascript" src="syntaxfest19_files/jquery-ui.js"></script>

<script type="text/javascript" src="syntaxfest19_files/actionsvg.js"></script>


<link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
<link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">



</head>

<body>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>

<table width="75%" cellspacing="0" cellpadding="10" border="0" bgcolor="white" align="center">
<tbody>
<tr>
<td valign="top">

<a href="index.html"> <img src="syntaxfest19_files/syntaxFest2019.png" style="width: 200px; padding-left: 25px;"></a>
<br><br><div><a title="Home" href="index.html" style="padding-left:25px;padding-right:55px;" class="nav">Home</a></div><br>

<br><div><a title="Quasy 19" href="https://quasy-2019.webnode.com/" style="padding-left:25px;padding-right:55px;" class="nav">Quasy website</a></div><br>
<br><div><a title="Depling 19" href="http://depling.org/depling2019/" style="padding-left:25px;padding-right:55px;" class="nav">Depling 19 website</a></div><br>
<br><div><a title="TLT 2019" href="tlt2019/tlt2019.html" style="padding-left:25px;;padding-right:55px;" class="nav">TLT 2019 website</a></div><br>
<br><div><a title="UDW 19" href="http://universaldependencies.org/udw19/" style="padding-left:25px;padding-right:55px;" class="nav">UDW 19 website</a></div><br>

<br><div><a title="Committee" href="syntaxfest19_committee.html" style="padding-left:25px;;padding-right:55px;" class="nav">Shared Program Committee</a></div><br><br>
<br><div><a title="Submission website" href="https://www.easychair.org/conferences/?conf=syntaxfest2019" style="padding-left:25px;;padding-right:55px;" class="nav">Submission site</a></div><br>
<br><div><a title="Registration website" href="http://boutique.univ-paris3.fr/syntaxfest/index.php?id_category=64&controller=category&id_lang=1" style="padding-left:25px;;padding-right:55px;" class="nav">Registration site</a></div><br>
<br><div><a title="Registration form" href="https://framaforms.org/syntaxfest-inscription-1561464051" style="padding-left:25px;;padding-right:55px;" class="nav">Registration form</a></div><br>
<br><div><a title="Accepted papers" href="accepted_papers.html" style="padding-left:25px;;padding-right:55px;" class="nav">Accepted papers</a></div><br>
<br><div><a title="Program" href="program.html" style="padding-left:25px;;padding-right:55px;" class="nav">Program (under construction)</a></div><br>

<!-- sponsoring (side bar)
<div style="clear:left; padding-top: 80px; padding-left:0"><href="http: www.ai-lc.it="" en="" "="">Sponsored by: <br><img src="syntaxfest19_files/AILC.png" alt="AILC" width="100%"></href="http:></div>
-->

</td>
<td rowspan="2" valign="top">

<!--<img style="width: 700px" src="syntaxfest19_files/logo.svg">

<hr style="background-color:#339980;height:10px;">

<br>-->

<h1> SyntaxFest 2019 <br>Invited talks</h1>



<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-ucm2{font-size:22px;font-family:"Arial Black", Gadget, sans-serif !important;;border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-xldj{border-color:inherit;text-align:left}
@media screen and (max-width: 767px) {.tg {width: auto !important;}.tg col {width: auto !important;}.tg-wrap {overflow-x: auto;-webkit-overflow-scrolling: touch;}}</style>
<div class="tg-wrap"><table class="tg">
  <tr>
    <td id="quasy-program" class="tg-ucm2" colspan="4"><span style="font-weight:bold;color:rgb(206, 99, 1)">Ramon Ferrer i Cancho (Universitat politècnica de Catalunya)</span></td>
  </tr>
  <tr>
    <td class="tg-xldj"><b>26th</b></td>
    <td class="tg-xldj">9:40</td>
    <td class="tg-xldj" colspan="2"><span style="font-weight:bold;color:rgb(49, 102, 255)">Title: </span><b>Dependency distance minimization: facts, theory and predictions</b><br><br><span style="font-weight:bold;color:rgb(49, 102, 255)">Abstract: </span>Quantitative linguistics is a branch of linguistics concerned about the study of statistical facts about languages and their explanation aiming at constructing a general theory of language. The quantitative study of syntax has become central to this branch of linguistics. The fact that the distance between syntactically related words is smaller than expected by chance in many languages led to the formulation of a dependency distance minimization (DDm) principle.<br><br>
From a theoretical standpoint, DDm is in conflict with another word order principle: surprisal minimization (Sm). In single head structures, DDm predicts that the head should be put at the center of the linear arrangement, while Sm predicts that it should be put at one of the ends.  In spite of the massive evidence of the action of DDm and the trendy claim that languages are optimized, attempts to quantify the degree of optimization of languages according to DDm have been rather scarce. Here we present a new optimality measure indicating that languages are optimized to a 70% on average.
We confirm two old theoretical predictions: that the action of DDm is stronger in longer sentences and that DDm is more likely to be beaten by Sm in short sequences (resulting in an anti-DDm effect), while shedding new light on the kind of tree structures where DDm is more likely to be shadowed. Finally, we review various theoretical predictions of DDm focusing on the scarcity of crossing dependencies. We challenge the belief that formal constraints on dependency trees (e.g., projectivity or relaxed versions) are real rather than epiphenomenal.  <br><br>
The talk is a summary of joint work with Carlos Gomez-Rodriguez, Juan Luis Esteban, Lluis Alemany-Puig and Xinying Chen.
</span>
<br><br>
<span style="font-weight:bold;color:rgb(49, 102, 255)">Short bio:</span>
</td>
  </tr>
  <tr>
    <td id="depling-program" class="tg-ucm2" colspan="4"><span style="color:rgb(206, 99, 1)">Emmanuel Dupoux</span></td>
  </tr>
  <tr>
    <td class="tg-xldj"><b>27th</b></td>
    <td class="tg-xldj">9:00</td>
    <td class="tg-xldj" colspan="2"><span style="font-weight:bold;color:rgb(49, 102, 255)">Title: </span><b>Inductive biases and language emergence in communicative agents</b><br><br><span style="font-weight:bold;color:rgb(49, 102, 255)">Abstract: </span>Despite spectacular progress in language modeling tasks, neural networks still fall short of the performance of human infants when it comes to learning a language from scarce and noisy data. Such performance presumably stems from human-specific inductive biases in the neural networks sustaining language acquisitions in the child. Here, we use two paradigms to study experimentally such inductive biases in artificial neural networks. The first one relies on iterative learning, where a sequence of agents learn from each other, simulating historical linguistic transmission. We find evidence that sequence to sequence neural models have some of the human inductive biases (like the preference for local dependencies), but lack others (like the preference for non-redundant markers of argument structure). The second paradigm relies on language emergence, where two agents engage in a communicative game. Here we find that sequence to sequence networks lack the preference for efficient communication found in humans, and in fact display an anti-Zipfian law of abbreviation. We conclude that the study of the inductive biases of neural networks is an important topic to improve the data efficiency of current systems.
</span>
<br><br>
<span style="font-weight:bold;color:rgb(49, 102, 255)">Short bio:</span> E. Dupoux directs the Cognitive Machine Learning team at the Ecole Normale Supérieure (ENS) in Paris and INRIA (www.syntheticlearner.com).  His education includes a PhD in Cognitive Science (EHESS), a MA in Computer Science (Orsay University) and a BA in Applied Mathematics (Pierre & Marie Curie University, ENS). His research mixes developmental science, cognitive neuroscience, and machine learning, with a focus on the reverse engineering of infant language and cognitive development using unsupervised or weakly supervised learning. He is the recipient of an Advanced ERC grant, the organizer of the Zero Ressource Speech Challenge (2015, 2017, 2019), the Intuitive Physics Benchmark (2019) and led in 2017 a Jelinek Summer Workshop at CMU on multimodal speech learning. He has authored 150 articles in peer reviewed outlets from both cognitive science and language technology.
</td>
  </tr>
  <tr>
    <td id="tlt-program" class="tg-ucm2" colspan="4"><span style="font-weight:bold;color:rgb(206, 99, 1)">Barbara Plank (IT University of Copenhagen)</span></td>
  </tr>
  <tr>
    <td class="tg-xldj"><br><b>28th</b></td>
    <td class="tg-xldj"><br>9:00</td>
    <td class="tg-xldj" colspan="2"><span style="font-weight:bold;color:rgb(49, 102, 255)">Title: </span><b>Transferring NLP models across languages and domains</b><br><br><span style="font-weight:bold;color:rgb(49, 102, 255)">Abstract: </span>How can we build Natural Language Processing models for new domains
and new languages? <br>In this talk I will survey some recent advances to
address this ubiquitous challenge, from cross-lingual transfer to
learning models under distant supervision from disparate sources,
      multitask-learning and data selection.</span>
<br><br>
<span style="font-weight:bold;color:rgb(49, 102, 255)">Short bio:</span>
</td>
  </tr>
  <tr>
    <td id="tlt-invited" class="tg-ucm2" colspan="4"><span style="color:rgb(206, 99, 1)">Paola Merlo (University of Geneva)</span></td>
  </tr>
  <tr>
    <td class="tg-xldj"><b>29th</b></td>
    <td class="tg-xldj">9:00</td>
    <td class="tg-xldj" colspan="2"><span style="font-weight:bold;color:rgb(49, 102, 255)">Title: </span><b>Quantitative Computational Syntax: dependencies, intervention effects and word embeddings</b><br><br><span style="font-weight:bold;color:rgb(49, 102, 255)">Abstract: </span>In the  computational study  of intelligent  behaviour, the  domain of
language is distinguished by the complexity of the representations and
the vast amounts  of quantitative text-driven  data. In this talk,  I will
let these two  aspects of the study of language  inform each other and
will  discuss  current  work   investigating  whether  the  notion  of
similarity  in  the intervention  theory  of  locality is  related  to
current notions of similarity in word embedding space.<br><br>

Despite   their  practical   success   and  impressive   performances,
neural-network-based and  distributed semantics techniques  have often
been criticized as  they remain fundamentally opaque  and difficult to
interpret.   Several  recent  pieces  of work  have  investigated  the
linguistic abilities of these representations, and shown that they can
capture long agreement  and thus hierarchical notions.   In this vein,
we  study another  core,  defining and  more  challenging property  of
language:  the ability  to  construe  long-distance dependencies.   We
present  results that  show that  word embeddings  and the  similarity
spaces  they define  do  not correlate  with  experimental results  on
intervention similarity  in long-distance dependencies.  These results
show that the linguistic  encoding in distributed representations does
not appear to be human-like, and it also brings evidence to the debate
on narrow  or broad definitions  of similarity in syntax  and sentence
      processing.</span>
<br><br>
<span style="font-weight:bold;color:rgb(49, 102, 255)">Short bio: </span>
Paola Merlo  is associate professor  in the Linguistics  department of
the University  of Geneva.  She  is the head of  the interdisciplinary
research  group Computational  Learning and  Computational Linguistics
(CLCL).   The  group  is  concerned  with  interdisciplinary  research
combining  linguistic  modelling  with  machine  learning  techniques.
Prof.  Merlo  has been editor of  Computational Linguistics, published
by  MIT  Press  and  a  member  of  the  executive  committee  of  the
ACL. Prof. Merlo holds a  doctorate in  Computational Linguistics  from the
University of Maryland,  and  has been  associate research fellow
at the  University of Pennsylvania,  and visiting scholar  at Rutgers,
Edinburgh, Stanford and Uppsala.</span>
</td>
  </tr>
  <tr>
    <td id="udw-invited" class="tg-ucm2" colspan="4"><span style="font-weight:bold;color:rgb(206, 99, 1)">Adam Przepiórkowski (Institute of Computer Science, Polish Academy of Sciences)</span></td>
  </tr>
  <tr>
    <td class="tg-xldj"><b>30th</b></td>
    <td class="tg-xldj">TBA</td>
    <td class="tg-xldj" colspan="2"><span style="font-weight:bold;color:rgb(49, 102, 255)">Title: </span><b>Grammatical functions</b><br><br><span style="font-weight:bold;color:rgb(49, 102, 255)">Abstract: </span>TBA</span>
<br><br>
<span style="font-weight:bold;color:rgb(49, 102, 255)">Short bio:</span>
</td>
  </tr>
</table></div>



</td>
</tr>
</tbody></table>




</body></html>
